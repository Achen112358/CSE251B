{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669cfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51810631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Load the configuration from config.yaml.\n",
    "    \"\"\"\n",
    "    return yaml.load(open('config.yaml', 'r'), Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "def normalize_data(inp, mu=None, sigma=None):\n",
    "    \"\"\"\n",
    "    TODO: Normalize your inputs here to have 0 mean and unit variance.\n",
    "    \"\"\"\n",
    "    inp = np.array(inp.copy())\n",
    "    n, m = inp.shape\n",
    "    inp = inp.reshape(n, 3, 1024).transpose(1,0,2).reshape(3, -1)\n",
    "    \n",
    "    if mu is None:\n",
    "        mu = np.mean(inp, axis=-1, keepdims=True)\n",
    "    if sigma is None:\n",
    "        sigma = np.std(inp, axis=-1, keepdims=True)\n",
    "        \n",
    "    inp = (inp - mu) / sigma\n",
    "    inp = inp.reshape(3, n, 1024).transpose(1,0,2).reshape(n, m)\n",
    "    \n",
    "    return inp, mu, sigma\n",
    "\n",
    "\n",
    "def one_hot_encoding(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    TODO: Encode labels using one hot encoding and return them.\n",
    "    \"\"\"\n",
    "    KI = np.eye(num_classes)\n",
    "    labels = KI[labels]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 data.\n",
    "    \"\"\"\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    cifar_path = os.path.join(path, \"cifar-10-batches-py\")\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(1,6):\n",
    "        images_dict = unpickle(os.path.join(cifar_path, f\"data_batch_{i}\"))\n",
    "        data = images_dict[b'data']\n",
    "        label = images_dict[b'labels']\n",
    "        labels.extend(label)\n",
    "        images.extend(data)\n",
    "    normalized_images_train, mu, sigma = normalize_data(images)\n",
    "    one_hot_labels_train    = one_hot_encoding(labels, num_classes=10) #(n,10)\n",
    "\n",
    "    test_images_dict = unpickle(os.path.join(cifar_path, f\"test_batch\"))\n",
    "    test_data = test_images_dict[b'data']\n",
    "    test_labels = test_images_dict[b'labels']\n",
    "    normalized_images_test, _, __ = normalize_data(test_data, mu, sigma)\n",
    "    one_hot_labels_test    = one_hot_encoding(test_labels, num_classes=10) #(n,10)\n",
    "    return  np.array(normalized_images_train), np.array(one_hot_labels_train), \\\n",
    "            np.array(normalized_images_test), np.array(one_hot_labels_test)\n",
    "    \n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    TODO: Implement the softmax function here.\n",
    "    Remember to take care of the overflow condition.\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x -= np.max(x, axis=-1, keepdims=True) #in case exp(x) is too large to be nan\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis = -1, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60cf396",
   "metadata": {},
   "source": [
    "### 3(a) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a66b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "X_train, y_train, X_test, y_test = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4aeb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b00ff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87082"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = knn.predict(X_test)\n",
    "np.mean([p==t for p, t in zip(pred, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9db996",
   "metadata": {},
   "source": [
    "### 3(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8fb7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 20\n",
    "num_class = 10\n",
    "inp_feature = 3072\n",
    "W_out = np.random.rand(J, num_class)\n",
    "W_hid = np.random.rand(inp_feature, J)\n",
    "\n",
    "def E(X, y, i=0, j=0, epsilon=0.01, mode='hidden'):\n",
    "    W_h = W_hid.copy()\n",
    "    W_o = W_out.copy()\n",
    "    if mode == 'hidden':\n",
    "        W_h[i, j] += epsilon\n",
    "    elif mode == 'output':\n",
    "        W_o[i,j] += epsilon\n",
    "        \n",
    "    Z_hidden = np.tanh(np.matmul(X, W_h))\n",
    "    output = softmax(np.matmul(Z_hidden, W_o))\n",
    "    loss = - np.mean(np.sum(y * np.log(output), axis = -1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c447a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0005336843116543477"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train[:10]\n",
    "y = y_train[:10]\n",
    "(E(X, y, epsilon=0.01) - E(X, y, epsilon=-0.01)) / 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78c0fb",
   "metadata": {},
   "source": [
    "### 3(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae2452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    \"\"\"\n",
    "    The class implements different types of activation functions for\n",
    "    your neural network layers.\n",
    "\n",
    "    Example (for sigmoid):\n",
    "        >>> sigmoid_layer = Activation(\"sigmoid\")\n",
    "        >>> z = sigmoid_layer(a)\n",
    "        >>> gradient = sigmoid_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_type = \"sigmoid\"):\n",
    "        \"\"\"\n",
    "        TODO: Initialize activation type and placeholders here.\n",
    "        \"\"\"\n",
    "        if activation_type not in [\"sigmoid\", \"tanh\", \"ReLU\", \"leakyReLU\"]:\n",
    "            raise NotImplementedError(f\"{activation_type} is not implemented.\")\n",
    "\n",
    "        # Type of non-linear activation.\n",
    "        self.activation_type = activation_type\n",
    "\n",
    "        # Placeholder for input. This will be used for computing gradients.\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, a):\n",
    "        \"\"\"\n",
    "        This method allows your instances to be callable.\n",
    "        \"\"\"\n",
    "        return self.forward(a)\n",
    "\n",
    "    def forward(self, a):\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "        \"\"\"\n",
    "        self.x = a.copy()\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.sigmoid(a)\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.tanh(a)\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.ReLU(a)\n",
    "\n",
    "        elif self.activation_type == \"leakyReLU\":\n",
    "            return self.leakyReLU(a)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Compute the backward pass.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            grad = self.grad_sigmoid()\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            grad = self.grad_tanh()\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            grad = self.grad_ReLU()\n",
    "        elif self.activation_type == \"leakyReLU\":\n",
    "            grad = self.grad_leakyReLU()\n",
    "            \n",
    "        return grad * delta\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement the sigmoid activation here.\n",
    "        \"\"\"\n",
    "        x = np.maximum(-700, x)\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement tanh here.\n",
    "        \"\"\"\n",
    "        return np.tanh(x) \n",
    "        \n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement ReLU here.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def leakyReLU(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement leaky ReLU here.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.maximum(x*0.1, x)\n",
    "\n",
    "    def grad_sigmoid(self):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for sigmoid here.\n",
    "        \"\"\"\n",
    "        x = np.maximum(-700, self.x)\n",
    "        val = self.sigmoid(x)\n",
    "        return  val * (1.0 - val) \n",
    "\n",
    "    def grad_tanh(self):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for tanh here.\n",
    "        \"\"\"\n",
    "        \n",
    "        return 1 - self.tanh(self.x)**2\n",
    "\n",
    "    def grad_ReLU(self):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for ReLU here.\n",
    "        \"\"\"\n",
    "        grad = self.x.copy()\n",
    "        grad[grad>0] = 1\n",
    "        grad[grad<0] = 0\n",
    "        return grad\n",
    "\n",
    "    def grad_leakyReLU(self):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for leaky ReLU here.\n",
    "        \"\"\"\n",
    "        grad = self.x.copy()\n",
    "        grad[grad>0] = 1\n",
    "        grad[grad<=0] = 0.1\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5a06384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    This class implements Fully Connected layers for your neural network.\n",
    "\n",
    "    Example:\n",
    "        >>> fully_connected_layer = Layer(784, 100)\n",
    "        >>> output = fully_connected_layer(input)\n",
    "        >>> gradient = fully_connected_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_units, out_units):\n",
    "        \"\"\"\n",
    "        Define the architecture and create placeholder.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.w = np.random.randn(in_units, out_units)    # Declare the Weight matrix\n",
    "        self.b = np.random.randn(1, out_units)    # Create a placeholder for Bias\n",
    "        self.x = None    # Save the input to forward in this\n",
    "        self.a = None    # Save the output of forward pass in this (without activation)\n",
    "\n",
    "        self.d_x = None  # Save the gradient w.r.t x in this\n",
    "        self.d_w = None  # Save the gradient w.r.t w in this\n",
    "        self.d_b = None  # Save the gradient w.r.t b in this\n",
    "        \n",
    "        self.d_w_old = 0\n",
    "        self.d_b_old = 0\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make layer callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the layer here.\n",
    "        DO NOT apply activation here.\n",
    "        Return self.a\n",
    "        \"\"\"\n",
    "        self.x = x.copy()\n",
    "        self.a = np.matmul(x, self.w) + self.b\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Write the code for backward pass. This takes in gradient from its next layer as input,\n",
    "        computes gradient for its weights and the delta to pass to its previous layers.\n",
    "        Return self.dx\n",
    "        \"\"\"\n",
    "        batch_size = self.x.shape[0]\n",
    "\n",
    "        self.d_x = np.matmul(delta, self.w.T)\n",
    "        self.d_w = np.matmul(self.x.T, delta) / (batch_size * 10)\n",
    "        self.d_b = np.mean(delta, axis=0) / 10\n",
    "\n",
    "        return self.d_x\n",
    "    \n",
    "    def update(self, lr, l2_penalty = 0, momentum = False, momentum_gamma = 1):\n",
    "        \n",
    "        # l2 penalty\n",
    "        d_w  = self.d_w + l2_penalty + self.w\n",
    "        \n",
    "        #momentum\n",
    "        d_w = momentum * d_w + (1-momentum) * self.d_w_old \n",
    "        d_b = momentum * self.d_b + (1-momentum) * self.d_b_old\n",
    "        self.d_w_old = d_w\n",
    "        self.d_b_old = d_b\n",
    "        \n",
    "        self.w -= lr * d_w\n",
    "        self.b -= lr * d_b\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c77f2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuralnetwork():\n",
    "    \"\"\"\n",
    "    Create a Neural Network specified by the input configuration.\n",
    "\n",
    "    Example:\n",
    "        >>> net = NeuralNetwork(config)\n",
    "        >>> output = net(input)\n",
    "        >>> net.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Create the Neural Network using config.\n",
    "        \"\"\"\n",
    "        self.layers = []     # Store all layers in this list.\n",
    "        self.x = None        # Save the input to forward in this\n",
    "        self.y = None        # Save the output vector of model in this\n",
    "        self.targets = None  # Save the targets in forward in this variable\n",
    "        self.l2_penalty = config['L2_penalty']\n",
    "        self.lr = config['learning_rate']\n",
    "        self.momentum = config['momentum']\n",
    "        self.momentum_gamma = config['momentum_gamma']\n",
    "\n",
    "        # Add layers specified by layer_specs.\n",
    "        for i in range(len(config['layer_specs']) - 1):\n",
    "            self.layers.append(Layer(config['layer_specs'][i], config['layer_specs'][i+1]))\n",
    "            if i < len(config['layer_specs']) - 2:\n",
    "                self.layers.append(Activation(config['activation']))\n",
    "\n",
    "    def __call__(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Make NeuralNetwork callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x, targets)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        TODO: Compute forward pass through all the layers in the network and return it.\n",
    "        If targets are provided, return loss as well.\n",
    "        \"\"\"\n",
    "        self.x = x.copy()\n",
    "        self.targets = targets\n",
    "        \n",
    "        inp = x.copy()\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "            \n",
    "        self.y = softmax(inp)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = self.loss(self.y, self.targets)\n",
    "            return self.y, loss\n",
    "        else:\n",
    "            return self.y\n",
    "            \n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        '''\n",
    "        TODO: compute the categorical cross-entropy loss and return it.\n",
    "        '''\n",
    "        loss = - np.mean(np.sum(np.log(logits + 1e-9) * targets, axis=-1))\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def backward(self):\n",
    "        '''\n",
    "        TODO: Implement backpropagation here.\n",
    "        Call backward methods of individual layers.\n",
    "        '''\n",
    "        delta = self.y - self.targets\n",
    "        for layer in self.layers[::-1]:\n",
    "            delta = layer.backward(delta)\n",
    "            \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.update(lr=self.lr, l2_penalty = self.l2_penalty, momentum = self.momentum, momentum_gamma = self.momentum_gamma)\n",
    "                \n",
    "    def predict_acc(self, x, targets):\n",
    "        y = self.forward(x)\n",
    "        predictions = np.argmax(y, axis=1)\n",
    "        targets = np.argmax(targets, axis=1)\n",
    "        return np.mean(predictions == targets)\n",
    "    \n",
    "    def predict(self, x, targets):\n",
    "        y = self.forward(x)\n",
    "        predictions = np.argmax(y, axis=1)\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cd7b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset):\n",
    "    X, y = dataset\n",
    "    n = len(X)\n",
    "    indexes = list(range(n))\n",
    "    random.shuffle(indexes)\n",
    "    return (X[indexes], y[indexes])\n",
    "\n",
    "def generate_minibatches(dataset, batch_size=64):\n",
    "    X, y = shuffle(dataset)\n",
    "    l_idx, r_idx = 0, batch_size\n",
    "    while r_idx < len(X):\n",
    "        yield X[l_idx:r_idx], y[l_idx:r_idx]\n",
    "        l_idx, r_idx = r_idx, r_idx + batch_size\n",
    "\n",
    "    yield X[l_idx:], y[l_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e345019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_valid, y_valid, config):\n",
    "    \"\"\"\n",
    "    TODO: Train your model here.\n",
    "    Implement batch SGD to train the model.\n",
    "    Implement Early Stopping.\n",
    "    Use config to set parameters for training like learning rate, momentum, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    early_stop_En = config['early_stop']\n",
    "    epoch_threshold = config['early_stop_epoch']\n",
    "    \n",
    "    start_time = time.time()\n",
    "        \n",
    "    best_acc = 0.0\n",
    "    patient = 0\n",
    "    for t in range(config['epochs']):\n",
    "        train_loss_batch, train_accuracy_batch = [], []\n",
    "        for x, y in generate_minibatches((x_train, y_train), batch_size=config['batch_size']):\n",
    "            opt, loss = model.forward(x, targets=y)\n",
    "            train_loss_batch.append(loss)\n",
    "            model.backward()\n",
    "            model.update()   \n",
    "            acc = model.predict_acc(x, y)\n",
    "            train_accuracy_batch.append(acc)\n",
    "            \n",
    "        \n",
    "        train_loss = np.mean(np.array(train_loss_batch))\n",
    "        train_accuracy = np.mean(np.array(train_accuracy_batch))\n",
    "        print(\"Epoch {}: Time cost {}s. Training loss is {}. Training Accuracy is {}.\".format(t + 1, round(time.time() - start_time, 2), round(train_loss,4), round(train_accuracy, 4)))\n",
    "        \n",
    "        \n",
    "        if (t+1) % 10 == 0:\n",
    "            valid_loss = model.forward(x_valid, targets=y_valid)[1]\n",
    "            valid_acc = model.predict_acc(x_valid, targets=y_valid)\n",
    "            print('Begin Validation! Time cost {}s. Validation loss is {}. Validation Accuracy is {}.'.format(round(time.time() - start_time, 2), round(valid_loss,4), round(valid_acc,4)))\n",
    "            if valid_acc > best_acc:\n",
    "                best_acc = valid_acc\n",
    "                patient = 0\n",
    "            else:\n",
    "                patient += 1\n",
    "            \n",
    "            if patient > config['early_stop_epoch']:\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbed44ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Time cost 4.82s. Training loss is 3.6528. Training Accuracy is 0.0966.\n",
      "Epoch 2: Time cost 9.12s. Training loss is 2.5446. Training Accuracy is 0.1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hx/tt9srqj121q8jyjtd_zh_y4w0000gn/T/ipykernel_77780/1368358827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/hx/tt9srqj121q8jyjtd_zh_y4w0000gn/T/ipykernel_77780/3343347352.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x_train, y_train, x_valid, y_valid, config)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mtrain_loss_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hx/tt9srqj121q8jyjtd_zh_y4w0000gn/T/ipykernel_77780/1913666633.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hx/tt9srqj121q8jyjtd_zh_y4w0000gn/T/ipykernel_77780/363840321.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \"\"\"\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = load_config(\"./\")\n",
    "model = Neuralnetwork(config)\n",
    "train(model, X_train, y_train, X_test, y_test, config)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "294f625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    \"\"\"\n",
    "    Load the sanity data to verify your implementation.\n",
    "    \"\"\"\n",
    "    return pickle.load(open(path + 'sanity.pkl', 'rb'))\n",
    "\n",
    "\n",
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Load the configuration from config.yaml.\n",
    "    \"\"\"\n",
    "    return yaml.load(open('config.yaml', 'r'), Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "def check_error(error, msg):\n",
    "    \"\"\"\n",
    "    Verify that error is below the threshold.\n",
    "    \"\"\"\n",
    "    if error < 1e-6:\n",
    "        print(f\"{msg} is CORRECT\")\n",
    "    else:\n",
    "        print(f\"{msg} is WRONG\")\n",
    "\n",
    "\n",
    "def sanity_layers(data):\n",
    "    \"\"\"\n",
    "    Check implementation of the forward and backward pass for all activations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the seed to reproduce results.\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Pseudo-input.\n",
    "    random_input = np.random.randn(1, 50)\n",
    "\n",
    "    # Get the activations.\n",
    "    act_sigmoid = Activation('sigmoid')\n",
    "    act_tanh    = Activation('tanh')\n",
    "    act_ReLU    = Activation('ReLU')\n",
    "    act_leakyReLU = Activation('leakyReLU')\n",
    "\n",
    "    # Get the outputs for forward-pass.\n",
    "    out_sigmoid = act_sigmoid(random_input)\n",
    "    out_tanh    = act_tanh(random_input)\n",
    "    out_ReLU    = act_ReLU(random_input)\n",
    "    out_leakyReLU = act_leakyReLU(random_input)\n",
    "\n",
    "    # Compute the errors.\n",
    "    err_sigmoid = np.sum(np.abs(data['out_sigmoid'] - out_sigmoid))\n",
    "    err_tanh    = np.sum(np.abs(data['out_tanh'] - out_tanh))\n",
    "    err_ReLU    = np.sum(np.abs(data['out_ReLU'] - out_ReLU))\n",
    "    err_leakyReLU = np.sum(np.abs(data['out_leakyReLU'] - out_leakyReLU))\n",
    "\n",
    "    # Check the errors.\n",
    "    check_error(err_sigmoid, \"Sigmoid Forward Pass\")\n",
    "    check_error(err_tanh,    \"Tanh Forward Pass\")\n",
    "    check_error(err_ReLU,    \"ReLU Forward Pass\")\n",
    "    check_error(err_leakyReLU,    \"leakyReLU Forward Pass\")\n",
    "\n",
    "    print(20 * \"-\", \"\\n\")\n",
    "\n",
    "    # Compute the gradients.\n",
    "    grad_sigmoid = act_sigmoid.backward(1.0)\n",
    "    grad_tanh    = act_tanh.backward(1.0)\n",
    "    grad_ReLU    = act_ReLU.backward(1.0)\n",
    "    grad_leakyReLU = act_leakyReLU.backward(1.0)\n",
    "\n",
    "    # Compute the errors.\n",
    "    err_sigmoid_grad = np.sum(np.abs(data['grad_sigmoid'] - grad_sigmoid))\n",
    "    err_tanh_grad    = np.sum(np.abs(data['grad_tanh'] - grad_tanh))\n",
    "    err_ReLU_grad    = np.sum(np.abs(data['grad_ReLU'] - grad_ReLU))\n",
    "    err_leakyReLU_grad = np.sum(np.abs(data['grad_leakyReLU'] - grad_leakyReLU))\n",
    "\n",
    "    # Check the errors.\n",
    "    check_error(err_sigmoid_grad, \"Sigmoid Gradient\")\n",
    "    check_error(err_tanh_grad,    \"Tanh Gradient\")\n",
    "    check_error(err_ReLU_grad,    \"ReLU Gradient\")\n",
    "    check_error(err_leakyReLU_grad, \"leakyReLU Gradient\")\n",
    "\n",
    "    print(20 * \"-\", \"\\n\")\n",
    "\n",
    "\n",
    "def sanity_network(data, default_config):\n",
    "    \"\"\"\n",
    "    Check implementation of the neural network's forward pass and backward pass.\n",
    "    \"\"\"    \n",
    "    # Set seed to reproduce results.\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Random input for our network.\n",
    "    random_image = np.random.randn(1, 784)\n",
    "\n",
    "    # Initialize the network using the default configuration\n",
    "    nnet = Neuralnetwork(default_config)\n",
    "\n",
    "    # Compute the forward pass.\n",
    "    nnet(random_image, targets = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "\n",
    "    # Compute the backward pass.\n",
    "    nnet.backward()\n",
    "\n",
    "    layer_no = 0\n",
    "    for layer_idx, layer in enumerate(nnet.layers):\n",
    "        if isinstance(layer, Layer):\n",
    "            layer_no += 1\n",
    "            error_x   = np.sum(np.abs(data['nnet'].layers[layer_idx].x   - layer.x))\n",
    "            error_w   = np.sum(np.abs(data['nnet'].layers[layer_idx].w   - layer.w))\n",
    "            error_b   = np.sum(np.abs(data['nnet'].layers[layer_idx].b   - layer.b))\n",
    "            error_d_w = np.sum(np.abs(data['nnet'].layers[layer_idx].d_w - layer.d_w))\n",
    "            error_d_b = np.sum(np.abs(data['nnet'].layers[layer_idx].d_b - layer.d_b))\n",
    "\n",
    "            check_error(error_x,   f\"Layer{layer_no}: Input\")\n",
    "            check_error(error_w,   f\"Layer{layer_no}: Weights\")\n",
    "            check_error(error_b,   f\"Layer{layer_no}: Biases\")\n",
    "            check_error(error_d_w, f\"Layer{layer_no}: Weight Gradient\")\n",
    "            check_error(error_d_b, f\"Layer{layer_no}: Bias Gradient\")\n",
    "\n",
    "    print(20 * \"-\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecf74b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Forward Pass is CORRECT\n",
      "Tanh Forward Pass is CORRECT\n",
      "ReLU Forward Pass is CORRECT\n",
      "leakyReLU Forward Pass is CORRECT\n",
      "-------------------- \n",
      "\n",
      "Sigmoid Gradient is CORRECT\n",
      "Tanh Gradient is CORRECT\n",
      "ReLU Gradient is CORRECT\n",
      "leakyReLU Gradient is CORRECT\n",
      "-------------------- \n",
      "\n",
      "Layer1: Input is CORRECT\n",
      "Layer1: Weights is CORRECT\n",
      "Layer1: Biases is CORRECT\n",
      "Layer1: Weight Gradient is CORRECT\n",
      "Layer1: Bias Gradient is CORRECT\n",
      "Layer2: Input is CORRECT\n",
      "Layer2: Weights is CORRECT\n",
      "Layer2: Biases is CORRECT\n",
      "Layer2: Weight Gradient is CORRECT\n",
      "Layer2: Bias Gradient is CORRECT\n",
      "Layer3: Input is CORRECT\n",
      "Layer3: Weights is CORRECT\n",
      "Layer3: Biases is CORRECT\n",
      "Layer3: Weight Gradient is CORRECT\n",
      "Layer3: Bias Gradient is CORRECT\n",
      "-------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_data    = get_data(\"./\")\n",
    "default_config = load_config(\"./\")\n",
    "\n",
    "# Run Sanity.\n",
    "sanity_layers(sanity_data)\n",
    "sanity_network(sanity_data, default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84e218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
